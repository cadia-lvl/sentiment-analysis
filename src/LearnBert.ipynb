{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3976366845.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install transformers\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n",
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Predicitons:\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# Text Classification with BERT\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"This movie was amazing!\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.argmax(outputs.logits, dim=1)\n",
    "print(\"The Predicitons:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the Attention Weights\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"BERT's attention mechanism is fascinating.\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "attention_weights = outputs.attentions\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.092895030975342\n"
     ]
    }
   ],
   "source": [
    "# Pretraining and MLM\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"BERT is a powerful language model.\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, add_special_tokens=True)\n",
    "outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "\n",
    "loss = outputs.loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1094,  0.2831, -0.2219,  ..., -0.4116,  0.2604,  0.4303],\n",
      "         [ 0.3717,  0.0461,  0.3014,  ..., -0.2603, -0.2263,  0.0782],\n",
      "         [-0.2072,  0.3525, -0.2668,  ..., -0.0065, -0.6920, -0.0067],\n",
      "         ...,\n",
      "         [ 0.5834,  0.4004,  0.4881,  ..., -0.1181,  0.2630, -0.1225],\n",
      "         [ 0.0716, -0.2377, -0.3874,  ...,  0.2981,  0.0436, -0.6352],\n",
      "         [ 0.8747,  0.4112, -0.3423,  ...,  0.2614, -0.4867, -0.2898]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Extracting Word Embeddings\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"BERT embeddings are fascinating.\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, add_special_tokens=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "word_embeddings = outputs.last_hidden_state\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3527, -1.1815, -0.3027,  ..., -0.1794,  0.0872,  0.5573],\n",
      "         [-0.3768,  0.1754,  0.3395,  ..., -0.0359,  0.2162, -1.2251],\n",
      "         [ 1.8406, -0.6458,  0.5841,  ..., -0.7345,  0.7542, -0.1614],\n",
      "         ...,\n",
      "         [ 1.0414, -0.7009,  1.0362,  ...,  1.0581, -0.3068, -1.4171],\n",
      "         [-0.8934, -0.8139, -0.3154,  ..., -0.3933, -0.6383,  0.0522],\n",
      "         [ 0.0143, -0.0423, -0.0131,  ...,  0.0044, -0.0140, -0.0394]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Fine-Tuning Intermediate Layers\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Advanced fine-tuning with BERT.\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "intermediate_layer = outputs.hidden_states[6]  # 7th layer\n",
    "print(intermediate_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 1.71MB/s]\n",
      "C:\\Users\\eysi\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\eysi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.46MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.98MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 473kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 499M/499M [00:53<00:00, 9.38MB/s] \n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0640,  0.1073, -0.0181,  ..., -0.0383, -0.0555, -0.0151],\n",
      "         [-0.0647,  0.0450, -0.0528,  ...,  0.0814, -0.1633, -0.0284],\n",
      "         [ 0.0398,  0.0657, -0.1046,  ..., -0.1364, -0.0650,  0.0560],\n",
      "         ...,\n",
      "         [ 0.0273,  0.0271, -0.0042,  ..., -0.1554, -0.0037,  0.1105],\n",
      "         [-0.0560,  0.1078, -0.0385,  ..., -0.0619, -0.0614, -0.0363],\n",
      "         [ 0.0064,  0.1383,  0.0011,  ...,  0.0994, -0.0593,  0.0169]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Using RoBERTa\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "text = \"RoBERTa is an advanced variant of BERT.\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "embeddings = outputs.last_hidden_state\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: [unused0]\n"
     ]
    }
   ],
   "source": [
    "# Text summarization with BERT\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "original_text = \"Long text for summarization...\"\n",
    "inputs = tokenizer(original_text, return_tensors='pt', padding=False, truncation=True)\n",
    "\n",
    "summary_logits = model(**inputs).logits\n",
    "summary = tokenizer.decode(torch.argmax(summary_logits, dim=1))\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Long Texts with BERT\n",
    "\n",
    "max_seq_length = 512  # Max token limit for BERT\n",
    "text = \"Long text to be handled...\"\n",
    "text_chunks = [text[i:i + max_seq_length] for i in range(0, len(text), max_seq_length)]\n",
    "\n",
    "for chunk in text_chunks:\n",
    "    inputs = tokenizer(chunk, return_tensors='pt', padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    # Process outputs for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eysi\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "C:\\Users\\eysi\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\amp\\autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\eysi\\HR\\FINAL\\sentiment-analysis\\src\\LearnBert.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/eysi/HR/FINAL/sentiment-analysis/src/LearnBert.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/eysi/HR/FINAL/sentiment-analysis/src/LearnBert.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/eysi/HR/FINAL/sentiment-analysis/src/LearnBert.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m scaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eysi/HR/FINAL/sentiment-analysis/src/LearnBert.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m scaler\u001b[39m.\u001b[39mstep(optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eysi/HR/FINAL/sentiment-analysis/src/LearnBert.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m scaler\u001b[39m.\u001b[39mupdate()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "# Mixed-Precision Training with BERT\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "with autocast():\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    loss = outputs.loss\n",
    "\n",
    "scaler.scale(loss).backward()\n",
    "scaler.step(optimizer)\n",
    "scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain Adaptation with BERT\n",
    "\n",
    "domain_data = load_domain_specific_data()  # Load domain-specific dataset\n",
    "domain_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "train_domain(domain_model, domain_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
