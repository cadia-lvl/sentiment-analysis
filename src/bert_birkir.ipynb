{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at mideind/IceBERT and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "model_name = \"mideind/IceBERT\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "def tokenize_data(data, tokenizer, max_len=512):\n",
    "    return tokenizer(\n",
    "        data.tolist(), padding=\"max_length\", truncation=True, max_length=max_len)\n",
    "\n",
    "df = pd.read_csv(\"../Google-without-lem.csv\")\n",
    "\n",
    "df = df.sample(n=10000, random_state=RANDOM_SEED)\n",
    "df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def convert(sentiment):\n",
    "    return 1 if sentiment == \"positive\" else 0\n",
    "\n",
    "\n",
    "df[\"sentiment\"] = df.sentiment.apply(convert)\n",
    "\n",
    "# show how many positive and negative reviews we have\n",
    "# print(df.sentiment.value_counts())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# df[\"review\"] = df.review.apply(lambda x: x.replace(\"_NEG\", \"\"))\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df[\"review\"], df[\"sentiment\"], test_size=0.3, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize the training data\n",
    "train_data = tokenize_data(X_train, tokenizer)\n",
    "\n",
    "# Tokenize the validation data\n",
    "val_data = tokenize_data(X_val, tokenizer)\n",
    "\n",
    "# Tokenize the test data\n",
    "test_data = tokenize_data(X_test, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])  # ensure labels are included\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = SentimentDataset(train_data, y_train)\n",
    "val_dataset = SentimentDataset(val_data, y_val)\n",
    "test_dataset = SentimentDataset(test_data, y_test)\n",
    "\n",
    "# Set GPU\n",
    "\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def log(self, logs: dict):\n",
    "#         # Get the last evaluation loss.\n",
    "#         eval_loss = self.evaluate()[\"eval_loss\"]\n",
    "#         logs[\"eval_loss\"] = eval_loss\n",
    "\n",
    "#         # Call the parent class log method to handle the rest.\n",
    "#         super().log(logs)\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"steps\",  # \"steps\" to evaluate each `logging_steps` or \"epoch\" to evaluate each epoch\n",
    "#     eval_steps=1000,  # Evaluation and Save happens every 500 steps\n",
    "#     num_train_epochs=10,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     warmup_steps=500,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir=\"./logs\",\n",
    "# )\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    return {\"acc\": (np.argmax(p.predictions, axis=1) == p.label_ids).mean()}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,  # This will ensure that the best model is loaded at the end of training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 500/8750 [02:11<36:16,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5666, 'learning_rate': 5e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  6%|▌         | 500/8750 [02:27<36:16,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4655722379684448, 'eval_acc': 0.84, 'eval_runtime': 16.371, 'eval_samples_per_second': 91.626, 'eval_steps_per_second': 11.484, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 1000/8750 [04:39<34:14,  3.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4836, 'learning_rate': 4.696969696969697e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 11%|█▏        | 1000/8750 [04:55<34:14,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4484061598777771, 'eval_acc': 0.842, 'eval_runtime': 16.4518, 'eval_samples_per_second': 91.175, 'eval_steps_per_second': 11.427, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1500/8750 [07:15<31:31,  3.83it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3884, 'learning_rate': 4.3939393939393944e-05, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 17%|█▋        | 1500/8750 [07:32<31:31,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5198267698287964, 'eval_acc': 0.8626666666666667, 'eval_runtime': 16.4529, 'eval_samples_per_second': 91.169, 'eval_steps_per_second': 11.427, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2000/8750 [09:45<29:55,  3.76it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3517, 'learning_rate': 4.0909090909090915e-05, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 23%|██▎       | 2000/8750 [10:02<29:55,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5042148232460022, 'eval_acc': 0.8613333333333333, 'eval_runtime': 16.9607, 'eval_samples_per_second': 88.44, 'eval_steps_per_second': 11.084, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2500/8750 [12:22<26:50,  3.88it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2978, 'learning_rate': 3.787878787878788e-05, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 29%|██▊       | 2500/8750 [12:38<26:50,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43802404403686523, 'eval_acc': 0.8846666666666667, 'eval_runtime': 15.9922, 'eval_samples_per_second': 93.796, 'eval_steps_per_second': 11.756, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3000/8750 [14:48<23:35,  4.06it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2428, 'learning_rate': 3.484848484848485e-05, 'epoch': 3.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 34%|███▍      | 3000/8750 [15:04<23:35,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4011671841144562, 'eval_acc': 0.8853333333333333, 'eval_runtime': 15.7889, 'eval_samples_per_second': 95.003, 'eval_steps_per_second': 11.907, 'epoch': 3.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 3500/8750 [17:21<22:26,  3.90it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2149, 'learning_rate': 3.181818181818182e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|████      | 3500/8750 [17:37<22:26,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4456855356693268, 'eval_acc': 0.9033333333333333, 'eval_runtime': 16.3114, 'eval_samples_per_second': 91.96, 'eval_steps_per_second': 11.526, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 4000/8750 [19:48<21:24,  3.70it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1469, 'learning_rate': 2.878787878787879e-05, 'epoch': 4.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 46%|████▌     | 4000/8750 [20:04<21:24,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.583526611328125, 'eval_acc': 0.8726666666666667, 'eval_runtime': 16.3477, 'eval_samples_per_second': 91.756, 'eval_steps_per_second': 11.5, 'epoch': 4.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 4500/8750 [22:22<18:14,  3.88it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1216, 'learning_rate': 2.575757575757576e-05, 'epoch': 5.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 51%|█████▏    | 4500/8750 [22:38<18:14,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5595088601112366, 'eval_acc': 0.888, 'eval_runtime': 16.3139, 'eval_samples_per_second': 91.946, 'eval_steps_per_second': 11.524, 'epoch': 5.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 4500/8750 [22:39<21:23,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1359.3772, 'train_samples_per_second': 51.494, 'train_steps_per_second': 6.437, 'train_loss': 0.31269213019477, 'epoch': 5.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:16<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4011671841144562, 'eval_acc': 0.8853333333333333, 'eval_runtime': 16.3446, 'eval_samples_per_second': 91.773, 'eval_steps_per_second': 11.502, 'epoch': 5.14}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./sentiment_model\\\\tokenizer_config.json',\n",
       " './sentiment_model\\\\special_tokens_map.json',\n",
       " './sentiment_model\\\\vocab.json',\n",
       " './sentiment_model\\\\merges.txt',\n",
       " './sentiment_model\\\\added_tokens.json',\n",
       " './sentiment_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "model.save_pretrained(\"./sentiment_model\")\n",
    "tokenizer.save_pretrained(\"./sentiment_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
