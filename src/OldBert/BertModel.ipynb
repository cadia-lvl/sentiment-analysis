{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\n",
    "    fname=\"aclImdb_v1.tar.gz\", origin=URL, untar=True, cache_dir=\".\", cache_subdir=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.utils.get_file(\n",
    "    fname=\"aclImdb_v1.tar.gz\", origin=URL, untar=True, cache_dir=\".\", cache_subdir=\"\"\n",
    ")\n",
    "# The shutil module offers a number of high-level\n",
    "# operations on files and collections of files.\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create main directory path (\"/aclImdb\")\n",
    "main_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n",
    "# Create sub directory path (\"/aclImdb/train\")\n",
    "train_dir = os.path.join(main_dir, \"train\")\n",
    "# Remove unsup folder since this is a supervised learning task\n",
    "remove_dir = os.path.join(train_dir, \"unsup\")\n",
    "shutil.rmtree(remove_dir)\n",
    "# View the final train folder\n",
    "print(os.listdir(train_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a training dataset and a validation\n",
    "# dataset from our \"aclImdb/train\" directory with a 80/20 split.\n",
    "train = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=30000, validation_split=0.2, subset=\"training\", seed=123\n",
    ")\n",
    "test = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=30000,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train.take(1):\n",
    "    train_feat = i[0].numpy()\n",
    "    train_lab = i[1].numpy()\n",
    "\n",
    "train = pd.DataFrame([train_feat, train_lab]).T\n",
    "train.columns = [\"DATA_COLUMN\", \"LABEL_COLUMN\"]\n",
    "train[\"DATA_COLUMN\"] = train[\"DATA_COLUMN\"].str.decode(\"utf-8\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in test.take(1):\n",
    "    test_feat = j[0].numpy()\n",
    "    test_lab = j[1].numpy()\n",
    "\n",
    "test = pd.DataFrame([test_feat, test_lab]).T\n",
    "test.columns = [\"DATA_COLUMN\", \"LABEL_COLUMN\"]\n",
    "test[\"DATA_COLUMN\"] = test[\"DATA_COLUMN\"].str.decode(\"utf-8\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN):\n",
    "    train_InputExamples = train.apply(\n",
    "        lambda x: InputExample(\n",
    "            guid=None,  # Globally unique ID for bookkeeping, unused in this case\n",
    "            text_a=x[DATA_COLUMN],\n",
    "            text_b=None,\n",
    "            label=x[LABEL_COLUMN],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    validation_InputExamples = test.apply(\n",
    "        lambda x: InputExample(\n",
    "            guid=None,  # Globally unique ID for bookkeeping, unused in this case\n",
    "            text_a=x[DATA_COLUMN],\n",
    "            text_b=None,\n",
    "            label=x[LABEL_COLUMN],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return train_InputExamples, validation_InputExamples\n",
    "\n",
    "    train_InputExamples, validation_InputExamples = convert_data_to_examples(\n",
    "        train, test, \"DATA_COLUMN\", \"LABEL_COLUMN\"\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = []  # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,  # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True,  # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (\n",
    "            input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"],\n",
    "            input_dict[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                label=e.label,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.int32,\n",
    "                \"attention_mask\": tf.int32,\n",
    "                \"token_type_ids\": tf.int32,\n",
    "            },\n",
    "            tf.int64,\n",
    "        ),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = \"DATA_COLUMN\"\n",
    "LABEL_COLUMN = \"LABEL_COLUMN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputExample(guid=None, text_a=\"Hello, world\", text_b=None, label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(\n",
    "    train, test, DATA_COLUMN, LABEL_COLUMN\n",
    ")\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(\n",
    "    list(validation_InputExamples), tokenizer\n",
    ")\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")],\n",
    ")\n",
    "\n",
    "model.fit(train_data, epochs=2, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "    \"This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good\",\n",
    "    \"One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_batch = tokenizer(\n",
    "    pred_sentences, max_length=128, padding=True, truncation=True, return_tensors=\"tf\"\n",
    ")\n",
    "tf_outputs = model(tf_batch)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "labels = [\"Negative\", \"Positive\"]\n",
    "label = tf.argmax(tf_predictions, axis=1)\n",
    "label = label.numpy()\n",
    "for i in range(len(pred_sentences)):\n",
    "    print(pred_sentences[i], \": \\n\", labels[label[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
