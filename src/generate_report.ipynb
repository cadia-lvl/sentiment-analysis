{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.5) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1111, 2)\n",
      "(209, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 21:36:36.359136: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-29 21:36:36.397315: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 21:36:37.154311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/olafurj/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.9777777777777777, 'recall': 0.88, 'f1-score': 0.9263157894736842, 'support': 50.0}, '1': {'precision': 0.8909090909090909, 'recall': 0.98, 'f1-score': 0.9333333333333333, 'support': 50.0}, 'accuracy': 0.93, 'macro avg': {'precision': 0.9343434343434343, 'recall': 0.9299999999999999, 'f1-score': 0.9298245614035088, 'support': 100.0}, 'weighted avg': {'precision': 0.9343434343434344, 'recall': 0.93, 'f1-score': 0.9298245614035088, 'support': 100.0}}\n",
      "acc: 0.9300, seed: 9728, i: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/olafurj/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.9767441860465116, 'recall': 0.84, 'f1-score': 0.9032258064516129, 'support': 50.0}, '1': {'precision': 0.8596491228070176, 'recall': 0.98, 'f1-score': 0.9158878504672898, 'support': 50.0}, 'accuracy': 0.91, 'macro avg': {'precision': 0.9181966544267646, 'recall': 0.9099999999999999, 'f1-score': 0.9095568284594513, 'support': 100.0}, 'weighted avg': {'precision': 0.9181966544267646, 'recall': 0.91, 'f1-score': 0.9095568284594513, 'support': 100.0}}\n",
      "acc: 0.9100, seed: 4150, i: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/olafurj/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.8979591836734694, 'recall': 0.88, 'f1-score': 0.888888888888889, 'support': 50.0}, '1': {'precision': 0.8823529411764706, 'recall': 0.9, 'f1-score': 0.8910891089108911, 'support': 50.0}, 'accuracy': 0.89, 'macro avg': {'precision': 0.89015606242497, 'recall': 0.89, 'f1-score': 0.88998899889989, 'support': 100.0}, 'weighted avg': {'precision': 0.89015606242497, 'recall': 0.89, 'f1-score': 0.88998899889989, 'support': 100.0}}\n",
      "acc: 0.8900, seed: 9291, i: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/olafurj/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 1.0, 'recall': 0.84, 'f1-score': 0.9130434782608696, 'support': 50.0}, '1': {'precision': 0.8620689655172413, 'recall': 1.0, 'f1-score': 0.9259259259259259, 'support': 50.0}, 'accuracy': 0.92, 'macro avg': {'precision': 0.9310344827586207, 'recall': 0.9199999999999999, 'f1-score': 0.9194847020933978, 'support': 100.0}, 'weighted avg': {'precision': 0.9310344827586207, 'recall': 0.92, 'f1-score': 0.9194847020933977, 'support': 100.0}}\n",
      "acc: 0.9200, seed: 8823, i: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/olafurj/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.9555555555555556, 'recall': 0.86, 'f1-score': 0.9052631578947369, 'support': 50.0}, '1': {'precision': 0.8727272727272727, 'recall': 0.96, 'f1-score': 0.9142857142857144, 'support': 50.0}, 'accuracy': 0.91, 'macro avg': {'precision': 0.9141414141414141, 'recall': 0.9099999999999999, 'f1-score': 0.9097744360902256, 'support': 100.0}, 'weighted avg': {'precision': 0.9141414141414141, 'recall': 0.91, 'f1-score': 0.9097744360902256, 'support': 100.0}}\n",
      "acc: 0.9100, seed: 7196, i: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/olafurj/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 1.0, 'recall': 0.88, 'f1-score': 0.9361702127659575, 'support': 50.0}, '1': {'precision': 0.8928571428571429, 'recall': 1.0, 'f1-score': 0.9433962264150945, 'support': 50.0}, 'accuracy': 0.94, 'macro avg': {'precision': 0.9464285714285714, 'recall': 0.94, 'f1-score': 0.939783219590526, 'support': 100.0}, 'weighted avg': {'precision': 0.9464285714285714, 'recall': 0.94, 'f1-score': 0.939783219590526, 'support': 100.0}}\n",
      "acc: 0.9400, seed: 1323, i: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/olafurj/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.9347826086956522, 'recall': 0.86, 'f1-score': 0.8958333333333334, 'support': 50.0}, '1': {'precision': 0.8703703703703703, 'recall': 0.94, 'f1-score': 0.9038461538461539, 'support': 50.0}, 'accuracy': 0.9, 'macro avg': {'precision': 0.9025764895330113, 'recall': 0.8999999999999999, 'f1-score': 0.8998397435897436, 'support': 100.0}, 'weighted avg': {'precision': 0.9025764895330113, 'recall': 0.9, 'f1-score': 0.8998397435897436, 'support': 100.0}}\n",
      "acc: 0.9000, seed: 8920, i: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/olafurj/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.9183673469387755, 'recall': 0.9, 'f1-score': 0.9090909090909091, 'support': 50.0}, '1': {'precision': 0.9019607843137255, 'recall': 0.92, 'f1-score': 0.9108910891089109, 'support': 50.0}, 'accuracy': 0.91, 'macro avg': {'precision': 0.9101640656262505, 'recall': 0.91, 'f1-score': 0.90999099909991, 'support': 100.0}, 'weighted avg': {'precision': 0.9101640656262506, 'recall': 0.91, 'f1-score': 0.90999099909991, 'support': 100.0}}\n",
      "acc: 0.9100, seed: 1666, i: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/olafurj/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    ")\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "from pprint import pprint\n",
    "import generate_classification_report as gcr\n",
    "#def call_model(X_all, y_all, folder, device):\n",
    "path1 = 'external-icelandic-reviews/data/imdb-split/hannes-reviews-reviews-with-sentiment.csv'\n",
    "path2 = 'external-icelandic-reviews/data/imdb-split/kvikmyndaryni-reviews-with-sentiment.csv'\n",
    "\n",
    "d1 = pd.read_csv(path1)\n",
    "d2 = pd.read_csv(path2)\n",
    "d1.drop(['num', 'rating', 'id'], axis=1, inplace=True)\n",
    "d2.drop(['movie', 'rating'], axis=1, inplace=True)\n",
    "\n",
    "print(d1.shape)\n",
    "print(d2.shape)\n",
    "\n",
    "df_orig = pd.merge(d1, d2, how='outer')\n",
    "\n",
    "#print(df.shape)\n",
    "#print(df.head())\n",
    "#print(df.value_counts())\n",
    "\n",
    "#all_pos1 = d1.where(lambda x: x['sentiment'] =='Positive')\n",
    "device = 'cuda'\n",
    "model = './electra-base-google-batch8-remove-noise-model/'\n",
    "\n",
    "\n",
    "total = 0\n",
    "for i in range(0, 10):\n",
    "    r = random.randint(0, 10000)\n",
    "    \n",
    "    fifty_negative = df_orig.where(lambda x: x['sentiment'] == 'Negative').dropna().sample(n=50, random_state=r)\n",
    "    fifty_positive = df_orig.where(lambda x: x['sentiment'] == 'Positive').dropna().sample(n=50, random_state=r)\n",
    "\n",
    "    new_df = pd.merge(fifty_negative, fifty_positive, on=['sentiment', 'review'], how='outer')\n",
    "    new_df.sentiment = new_df.sentiment.apply(lambda x: 1 if x == 'Positive' else 0)\n",
    "    X_all = new_df.review\n",
    "    y_all = new_df.sentiment\n",
    "    print(X_all)\n",
    "    print(y_all)\n",
    "    accuracy = gcr.call_model(X_all, y_all, model, device)\n",
    "    total += accuracy\n",
    "    print('acc: {0:.4f}, seed: {1}, i: {2}'.format(accuracy, r, i))\n",
    "\n",
    "    \n",
    "print('Average accuracy: ', total/10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
