{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109483778 (417.65 MB)\n",
      "Trainable params: 109483778 (417.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\data_utils.py:179\u001b[0m, in \u001b[0;36m_extract_archive\u001b[1;34m(file_path, path, archive_format)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m         \u001b[39m# Tar archive, perhaps unsafe. Filter paths.\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m         archive\u001b[39m.\u001b[39;49mextractall(\n\u001b[0;32m    180\u001b[0m             path, members\u001b[39m=\u001b[39;49m_filter_safe_paths(archive)\n\u001b[0;32m    181\u001b[0m         )\n\u001b[0;32m    182\u001b[0m \u001b[39mexcept\u001b[39;00m (tarfile\u001b[39m.\u001b[39mTarError, \u001b[39mRuntimeError\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tarfile.py:2255\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[1;34m(self, path, members, numeric_owner, filter)\u001b[0m\n\u001b[0;32m   2253\u001b[0m     members \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m-> 2255\u001b[0m \u001b[39mfor\u001b[39;00m member \u001b[39min\u001b[39;00m members:\n\u001b[0;32m   2256\u001b[0m     tarinfo \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_extract_tarinfo(member, filter_function, path)\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\data_utils.py:123\u001b[0m, in \u001b[0;36m_filter_safe_paths\u001b[1;34m(members)\u001b[0m\n\u001b[0;32m    122\u001b[0m valid_path \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[39mif\u001b[39;00m _is_path_in_dir(finfo\u001b[39m.\u001b[39;49mname, base_dir):\n\u001b[0;32m    124\u001b[0m     valid_path \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\data_utils.py:111\u001b[0m, in \u001b[0;36m_is_path_in_dir\u001b[1;34m(path, base_dir)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_is_path_in_dir\u001b[39m(path, base_dir):\n\u001b[1;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m _resolve_path(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(base_dir, path))\u001b[39m.\u001b[39mstartswith(base_dir)\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\data_utils.py:107\u001b[0m, in \u001b[0;36m_resolve_path\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_resolve_path\u001b[39m(path):\n\u001b[1;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mrealpath(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mabspath(path))\n",
      "File \u001b[1;32m<frozen ntpath>:696\u001b[0m, in \u001b[0;36mrealpath\u001b[1;34m(path, strict)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Notandi\\RU\\FINAL\\sentiment-analysis\\src\\BertModel.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m URL \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mget_file(fname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39maclImdb_v1.tar.gz\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                   origin\u001b[39m=\u001b[39;49mURL,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                   untar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                   cache_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                   cache_subdir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\data_utils.py:371\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39mif\u001b[39;00m untar:\n\u001b[0;32m    370\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(untar_fpath):\n\u001b[1;32m--> 371\u001b[0m         _extract_archive(fpath, datadir, archive_format\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtar\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    372\u001b[0m     \u001b[39mreturn\u001b[39;00m untar_fpath\n\u001b[0;32m    374\u001b[0m \u001b[39mif\u001b[39;00m extract:\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\data_utils.py:187\u001b[0m, in \u001b[0;36m_extract_archive\u001b[1;34m(file_path, path, archive_format)\u001b[0m\n\u001b[0;32m    185\u001b[0m                 os\u001b[39m.\u001b[39mremove(path)\n\u001b[0;32m    186\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m                 shutil\u001b[39m.\u001b[39;49mrmtree(path)\n\u001b[0;32m    188\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:759\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror, dir_fd)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[39m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m \u001b[39mreturn\u001b[39;00m _rmtree_unsafe(path, onerror)\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:617\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    615\u001b[0m         onerror(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mislink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n\u001b[0;32m    616\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 617\u001b[0m     _rmtree_unsafe(fullname, onerror)\n\u001b[0;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:617\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    615\u001b[0m         onerror(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mislink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n\u001b[0;32m    616\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 617\u001b[0m     _rmtree_unsafe(fullname, onerror)\n\u001b[0;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:617\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    615\u001b[0m         onerror(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mislink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n\u001b[0;32m    616\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 617\u001b[0m     _rmtree_unsafe(fullname, onerror)\n\u001b[0;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:620\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 620\u001b[0m         os\u001b[39m.\u001b[39munlink(fullname)\n\u001b[0;32m    621\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    622\u001b[0m         onerror(os\u001b[39m.\u001b[39munlink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\", \n",
    "                                  origin=URL,\n",
    "                                  untar=True,\n",
    "                                  cache_dir='.',\n",
    "                                  cache_subdir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labeledBow.feat', 'neg', 'pos', 'unsupBow.feat', 'urls_neg.txt', 'urls_pos.txt', 'urls_unsup.txt']\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\", \n",
    "                                  origin=URL,\n",
    "                                  untar=True,\n",
    "                                  cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "# The shutil module offers a number of high-level \n",
    "# operations on files and collections of files.\n",
    "import os\n",
    "import shutil\n",
    "# Create main directory path (\"/aclImdb\")\n",
    "main_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "# Create sub directory path (\"/aclImdb/train\")\n",
    "train_dir = os.path.join(main_dir, 'train')\n",
    "# Remove unsup folder since this is a supervised learning task\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)\n",
    "# View the final train folder\n",
    "print(os.listdir(train_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# We create a training dataset and a validation \n",
    "# dataset from our \"aclImdb/train\" directory with a 80/20 split.\n",
    "train = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=30000, validation_split=0.2, \n",
    "    subset='training', seed=123)\n",
    "test = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=30000, validation_split=0.2, \n",
    "    subset='validation', seed=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA_COLUMN</th>\n",
       "      <th>LABEL_COLUMN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canadian director Vincenzo Natali took the art...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I gave this film 10 not because it is a superb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I admit to being somewhat jaded about the movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For a long time, 'The Menagerie' was my favori...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A truly frightening film. Feels as if it were ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DATA_COLUMN LABEL_COLUMN\n",
       "0  Canadian director Vincenzo Natali took the art...            1\n",
       "1  I gave this film 10 not because it is a superb...            1\n",
       "2  I admit to being somewhat jaded about the movi...            1\n",
       "3  For a long time, 'The Menagerie' was my favori...            1\n",
       "4  A truly frightening film. Feels as if it were ...            0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in train.take(1):\n",
    "  train_feat = i[0].numpy()\n",
    "  train_lab = i[1].numpy()\n",
    "\n",
    "train = pd.DataFrame([train_feat, train_lab]).T\n",
    "train.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "train['DATA_COLUMN'] = train['DATA_COLUMN'].str.decode(\"utf-8\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA_COLUMN</th>\n",
       "      <th>LABEL_COLUMN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I can't believe that so much talent can be was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie blows - let's get that straight rig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The saddest thing about this \"tribute\" is that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm only rating this film as a 3 out of pity b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Something surprised me about this movie - it w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DATA_COLUMN LABEL_COLUMN\n",
       "0  I can't believe that so much talent can be was...            0\n",
       "1  This movie blows - let's get that straight rig...            0\n",
       "2  The saddest thing about this \"tribute\" is that...            0\n",
       "3  I'm only rating this film as a 3 out of pity b...            0\n",
       "4  Something surprised me about this movie - it w...            1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for j in test.take(1):\n",
    "  test_feat = j[0].numpy()\n",
    "  test_lab = j[1].numpy()\n",
    "\n",
    "test = pd.DataFrame([test_feat, test_lab]).T\n",
    "test.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "test['DATA_COLUMN'] = test['DATA_COLUMN'].str.decode(\"utf-8\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "  return train_InputExamples, validation_InputExamples\n",
    "\n",
    "  train_InputExamples, validation_InputExamples = convert_data_to_examples(train, \n",
    "                                                                           test, \n",
    "                                                                           'DATA_COLUMN', \n",
    "                                                                           'LABEL_COLUMN')\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = 'DATA_COLUMN'\n",
    "LABEL_COLUMN = 'LABEL_COLUMN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InputExample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Notandi\\RU\\FINAL\\sentiment-analysis\\src\\BertModel.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m InputExample(guid\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m              text_a \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHello, world\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m              text_b \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m              label \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'InputExample' is not defined"
     ]
    }
   ],
   "source": [
    "def InputExample(guid=None,\n",
    "             text_a = \"Hello, world\",\n",
    "             text_b = None,\n",
    "             label = 1):\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InputExample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Notandi\\RU\\FINAL\\sentiment-analysis\\src\\BertModel.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_InputExamples, validation_InputExamples \u001b[39m=\u001b[39m convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_data \u001b[39m=\u001b[39m convert_examples_to_tf_dataset(\u001b[39mlist\u001b[39m(train_InputExamples), tokenizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_data \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39mshuffle(\u001b[39m100\u001b[39m)\u001b[39m.\u001b[39mbatch(\u001b[39m32\u001b[39m)\u001b[39m.\u001b[39mrepeat(\u001b[39m2\u001b[39m)\n",
      "\u001b[1;32md:\\Notandi\\RU\\FINAL\\sentiment-analysis\\src\\BertModel.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_data_to_examples\u001b[39m(train, test, DATA_COLUMN, LABEL_COLUMN): \n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m   train_InputExamples \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: InputExample(guid\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, \u001b[39m# Globally unique ID for bookkeeping, unused in this case\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                                           text_a \u001b[39m=\u001b[39;49m x[DATA_COLUMN], \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                                           text_b \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                                           label \u001b[39m=\u001b[39;49m x[LABEL_COLUMN]), axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   validation_InputExamples \u001b[39m=\u001b[39m test\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: InputExample(guid\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m# Globally unique ID for bookkeeping, unused in this case\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                                           text_a \u001b[39m=\u001b[39m x[DATA_COLUMN], \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                                           text_b \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                                           label \u001b[39m=\u001b[39m x[LABEL_COLUMN]), axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m train_InputExamples, validation_InputExamples\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:10037\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m  10025\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10027\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m  10028\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m  10029\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10035\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m  10036\u001b[0m )\n\u001b[1;32m> 10037\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    835\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 837\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:963\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 963\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    965\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    966\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\Notandi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:979\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    977\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    978\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 979\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(v, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[0;32m    980\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    981\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    982\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    983\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32md:\\Notandi\\RU\\FINAL\\sentiment-analysis\\src\\BertModel.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_data_to_examples\u001b[39m(train, test, DATA_COLUMN, LABEL_COLUMN): \n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m   train_InputExamples \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: InputExample(guid\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m# Globally unique ID for bookkeeping, unused in this case\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                                           text_a \u001b[39m=\u001b[39m x[DATA_COLUMN], \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                                           text_b \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                                           label \u001b[39m=\u001b[39m x[LABEL_COLUMN]), axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   validation_InputExamples \u001b[39m=\u001b[39m test\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: InputExample(guid\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m# Globally unique ID for bookkeeping, unused in this case\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                                           text_a \u001b[39m=\u001b[39m x[DATA_COLUMN], \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                                           text_b \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                                           label \u001b[39m=\u001b[39m x[LABEL_COLUMN]), axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Notandi/RU/FINAL/sentiment-analysis/src/BertModel.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m train_InputExamples, validation_InputExamples\n",
      "\u001b[1;31mNameError\u001b[0m: name 'InputExample' is not defined"
     ]
    }
   ],
   "source": [
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs=2, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = ['This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good',\n",
    "                  'One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "tf_outputs = model(tf_batch)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "labels = ['Negative','Positive']\n",
    "label = tf.argmax(tf_predictions, axis=1)\n",
    "label = label.numpy()\n",
    "for i in range(len(pred_sentences)):\n",
    "  print(pred_sentences[i], \": \\n\", labels[label[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
